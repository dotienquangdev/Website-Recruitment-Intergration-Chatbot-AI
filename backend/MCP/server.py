import sys
import os
from pathlib import Path 


# Add the backend directory to Python path
backend_dir = Path(__file__).parent.parent
sys.path.insert(0, str(backend_dir))

# from mcp.server.fastmcp import FastMCP 
from mcp.server.fastmcp import FastMCP 
# from fastmcp import FastMCP




from typing import List, Dict, Any
from setting import Settings


# 1Ô∏è‚É£ T·∫°o server
server = FastMCP("demo-mcp")

# Load settings
settings = Settings.load_settings()


# üöÄ Preload models ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô response
print("üöÄ Initializing models...")
from tool.model_manager import model_manager
model_manager.preload_models()
print("‚úÖ Models preloaded successfully!")

# 2Ô∏è‚É£ ƒê·ªãnh nghƒ©a tool
@server.tool()
def hello(name: str) -> str:
    """Say hello to a user"""
    return f"Hello, {name}!"


@server.tool()
def intent_classification(query: str) -> str:
    """
    Ph√¢n lo·∫°i intent c·ªßa c√¢u h·ªèi (s·ª≠ d·ª•ng cached models)
    Args:
        query: c√¢u h·ªèi c·ªßa user
    Returns:
        str: intent ƒë√£ ph√¢n lo·∫°i (vd: "recruitment", "salary", "company_info", ...)
    """
    from tool.model_manager import model_manager
    
    try:
        # L·∫•y semantic router t·ª´ cache
        semantic_router = model_manager.get_semantic_router()
        
        # Ph√¢n lo·∫°i intent
        print(f"\nüîç Classifying query: {query}")
        score, route_name = semantic_router.guide(query)
        print(f"‚úÖ Classification result: {route_name} (score: {score:.4f})")
        
        return route_name
        
    except Exception as e:
        print(f"‚ùå Error in intent classification: {str(e)}")
        return "unknown"

@server.tool()
def get_reflection(history: List[Dict[str, str]]) -> str:
    """
    S·ª≠ d·ª•ng Reflection ƒë·ªÉ t·ª± ƒë√°nh gi√° v√† c·∫£i thi·ªán c√¢u tr·∫£ l·ªùi
    Args:
        history: l·ªãch s·ª≠ h·ªôi tho·∫°i
        question: c√¢u h·ªèi hi·ªán t·∫°i
        max_iterations: s·ªë l·∫ßn l·∫∑p t·ªëi ƒëa ƒë·ªÉ c·∫£i thi·ªán c√¢u tr·∫£ l·ªùi
    Returns:
        str: c√¢u tr·∫£ l·ªùi ƒë√£ ƒë∆∞·ª£c c·∫£i thi·ªán
    """
    from tool.reflection import Reflection
    from llms.llm_manager import llm_manager
    
    # S·ª≠ d·ª•ng LLM Manager thay v√¨ t·∫°o instance m·ªõi
    # Always use localhost - no Docker support
    default_url = "http://localhost:11434"
    ollama_url = os.getenv("OLLAMA_URL") or settings.OLLAMA_BASE_URL or default_url
    ollama_model = settings.OLLAMA_MODEL
    
    # Reuse existing LLM instance t·ª´ manager
    llm = llm_manager.get_ollama_client(base_url=ollama_url, model_name=ollama_model)
    reflection = Reflection(llm=llm)
    
    try:
        improved_answer = reflection.__call__(history)
        if "<think>" in improved_answer:
                improved_answer = improved_answer.split("</think>")[-1].strip()
        print("Reflection completed.", {"improved_answer": improved_answer})
        return improved_answer
    except Exception as e:
        print(f"‚ùå Error in reflection process: {str(e)}")
        return "Error in reflection process."

@server.tool()
def extract_features_cv(user_input: str) -> str:
    """
    Tr√≠ch xu·∫•t th√¥ng tin t·ª´ CV v√† ph√°t hi·ªán gian l·∫≠n
    Args:
        user_input: vƒÉn b·∫£n CV
    Returns:
        str: JSON h·ª£p l·ªá v·ªõi c√°c tr∆∞·ªùng th√¥ng tin v√† danh s√°ch red_flags (n·∫øu c√≥)
    """
    from llms.llm_manager import llm_manager
    from prompt.promt_config import PromptConfig
    
    # S·ª≠ d·ª•ng LLM Manager thay v√¨ t·∫°o instance m·ªõi
    # Always use localhost - no Docker support
    default_url = "http://localhost:11434"
    ollama_url = os.getenv("OLLAMA_URL") or settings.OLLAMA_BASE_URL or default_url
    ollama_model = settings.OLLAMA_MODEL
    
    # Reuse existing LLM instance t·ª´ manager
    llm = llm_manager.get_ollama_client(base_url=ollama_url, model_name=ollama_model)
    
    try:
        prompt_config = PromptConfig()
        prompt = prompt_config.get_prompt("extract_features_cv", user_input=user_input)
        print(f"\nüîç Extracting features from CV...")
        response = llm.generate_content([{"role": "user", "content": prompt}])
        print("‚úÖ Feature extraction completed.")
        return response
    except Exception as e:
        print(f"‚ùå Error in feature extraction: {str(e)}")
        return "Error in feature extraction."

@server.tool()
def get_reflection_openai(history: List[Dict[str, str]]) -> str:
    """
    S·ª≠ d·ª•ng Reflection v·ªõi OpenAI API ƒë·ªÉ t·ª± ƒë√°nh gi√° v√† c·∫£i thi·ªán c√¢u tr·∫£ l·ªùi
    Args:
        history: l·ªãch s·ª≠ h·ªôi tho·∫°i
    Returns:
        str: c√¢u tr·∫£ l·ªùi ƒë√£ ƒë∆∞·ª£c c·∫£i thi·ªán
    """
    from tool.reflection import Reflection
    from openai import OpenAI
    
    # T·∫°o OpenAI client v·ªõi settings
    client = OpenAI(
        base_url=settings.BASE_URL_OPENAI,
        api_key=settings.API_KEY_OPENAI
    )
    
    # T·∫°o wrapper class ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi Reflection
    class OpenAIWrapper:
        def __init__(self, client, model_name):
            self.client = client
            self.model_name = model_name
        
        def generate_content(self, messages):
            try:
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=messages
                )
                return response.choices[0].message.content
            except Exception as e:
                print(f"‚ùå Error generating content with OpenAI: {str(e)}")
                raise
    
    # T·∫°o wrapper v·ªõi model t·ª´ settings
    openai_wrapper = OpenAIWrapper(client, settings.MODE_KAT_CODER)
    reflection = Reflection(llm=openai_wrapper)
    
    try:
        improved_answer = reflection.__call__(history)
        if "<think>" in improved_answer:
                improved_answer = improved_answer.split("</think>")[-1].strip()
        print("OpenAI Reflection completed.", {"improved_answer": improved_answer})
        return improved_answer
    except Exception as e:
        print(f"‚ùå Error in OpenAI reflection process: {str(e)}")
        return "Error in OpenAI reflection process."
    
    
@server.tool()
def retrive_infor_company(query: str) -> List[Dict[str, Any]]:
    """
    Truy xu·∫•t th√¥ng tin c√¥ng ty t·ª´ Qdrant d·ª±a tr√™n c√¢u h·ªèi c·ªßa user
    Args:
        query: c√¢u h·ªèi c·ªßa user
    Returns:
        List[Dict]: danh s√°ch c√¥ng ty li√™n quan

    """
    from qdrant_client.models import Filter, FieldCondition, MatchValue
    try:

        # L·∫•y Qdrant client
        from tool.database import QDrant
        qdrant_client = QDrant(Settings=settings)
        
        scroll_filter = Filter(
        must=[
        FieldCondition(
            key="entity_type",
            match=MatchValue(value="company")
        ),
        ]
        )
        

        # T√¨m ki·∫øm trong Qdrant
        results = qdrant_client.search_vectors_with_filter(settings, query, "entities", top_k=7, filter=scroll_filter)
        
        # Tr√≠ch xu·∫•t th√¥ng tin c√¥ng ty t·ª´ k·∫øt qu·∫£
        companies = []
        for res in results:
            payload = res.payload
            if payload:
                companies.append(payload)
        
        print(f"‚úÖ Retrieved {len(companies)} companies related to the query.")
        return companies
        
    except Exception as e:
        print(f"‚ùå Error retrieving company info: {str(e)}")
        return []
    
@server.tool()
def retrive_infor_job_posting(query: str) -> List[Dict[str, Any]]:
    """
    Truy xu·∫•t th√¥ng tin job posting t·ª´ Qdrant d·ª±a tr√™n c√¢u h·ªèi c·ªßa user
    Args:
        query: c√¢u h·ªèi c·ªßa user
    Returns:
        List[Dict]: danh s√°ch job posting li√™n quan

    """
    from qdrant_client.models import Filter, FieldCondition, MatchValue
    try:

        # L·∫•y Qdrant client
        from tool.database import QDrant
        qdrant_client = QDrant(Settings=settings)
        
        scroll_filter = Filter(
        must=[
        FieldCondition(
            key="entity_type",
            match=MatchValue(value="job_posting")
        ),
        ]
        )
        
        # T√¨m ki·∫øm trong Qdrant
        results = qdrant_client.search_vectors_with_filter(settings, query, "entities", top_k=7, filter=scroll_filter)
        
        # Tr√≠ch xu·∫•t th√¥ng tin job posting t·ª´ k·∫øt qu·∫£
        job_postings = []
        for res in results:
            payload = res.payload
            if payload:
                job_postings.append(payload)
        
        print(f"‚úÖ Retrieved {len(job_postings)} job postings related to the query.")
        return job_postings
        
    except Exception as e:
        print(f"‚ùå Error retrieving job posting info: {str(e)}")
        return []



# 3Ô∏è‚É£ Ch·∫°y server qua STDIO
if __name__ == "__main__":
    server.run()
